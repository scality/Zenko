# Default values for zenko.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# By default, MongoDB, Redis-HA, Zenko-Queue, and Zenko-Quorum
# will use this value for their replica count. Typically, this
# is equivalent to the number of nodes in a Kubernetes Cluster.
nodeCount: &nodeCount 3



ingress:
  enabled: false

  # ingress.hosts has been deprecated in favor of ingress.root_domain and
  # ingress.*_sudomain values and may be removed in future versions.
  hosts:
    - zenko.local

  # Used to create an Ingress record.
  # This must match the 'cloudserver' 'endpoint', unless your client
  # supports different hostnames.
  # The root domain for endpoints
  root_domain: zenko.local

  # Subdomain to serve cloudserver traffic
  # Will be added as a prefix to root_domain
  # eg s3.example.com
  s3_subdomain: s3
  s3_extra_hosts: []

  # Subdomain to serve blobserver traffic
  # Will be added as a prefix to root_domain
  # eg blob.example.com
  blob_subdomain: blob
  blob_mgmt_subdomain: jabba
  blob_extra_hosts: []

  max_body_size: 100m
  # annotations:
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  # Enable deployment of cert-manager for automatic certificate issuance
  auto_certificates: False
  # Set the issuer for the created ingress
  # Has no effect if `auto_certificates` is false
  issuer: ''
  cluster_issuer: ''
  tls:
    # Secrets must be manually created in the namespace.
    # - secretName: zenko-tls
    #   hosts:
    #     - zenko.example.com

  # Adds a wildcard subdomain to the ingress for s3 and blob
  # Example: *.s3.zenko.local
  enable_dns_style_buckets: False

global:
  orbit:
    enabled: true
    endpoint: https://api.zenko.io
    # When 'orbit.enabled' is 'true', these aren't used, please use
    # https://zenko.io to manage your deployment
  locationConstraints: {}
  replicationEndpoints: []

s3Utils:
  image:
    repository: registry.scality.com/s3utils/s3utils
    tag: "1.9.0"
    pullPolicy: IfNotPresent

maintenance:
  # Enables a retry CronJobs that will attempt to retry CRR objects stuck in
  # pending or failed state based off the specified schedule. For Orbit enabled
  # installs, Zenko will need to be registered and account credentials
  # (secretKey and accessKey) will need to be provisioned prior to enabling this
  # maintenance feature.
  enabled: false
  accessKey: ""
  secretKey: ""
  successfulJobsHistory: 1
  retryFailed:
    schedule: "*/10 * * * *"
  retryPending:
    schedule: "*/10 * * * *"
  debug:
    # This provisions a pod with utilities to help debug Zenko from within the
    # cluster. Credentials can be added here that will be applied to the pods
    # environment variables and can be used for debugging but not necessary to
    # instanciate the pod.
    enabled: false
    accessKey: ""
    secretKey: ""
  kafkaClient:
    image:
      repository: solsson/kafka
      tag: 0.11.0.0
      pullPolicy: IfNotPresent
# Enables reporting cronjobs that are essential for displaying information
# in Orbit like total number of objects in the namespace
reporting:
  enabled: true
  successfulJobsHistory: 1
  countItems:
    schedule: "@hourly"

cloudserver:
  replicaCount: *nodeCount
  replicaFactor: 10
  mongodb:
    replicas: *nodeCount
  endpoint: zenko.local
  users: {}
    # accountName:
      # access:
      # secret:
      #
      #

backbeat:
  api:
    autoscaling:
      enabled: true
      config:
        maxReplicas: *nodeCount
    resources:
      requests:
        cpu: 250m
      limits:
        cpu: 1
  replication:
    dataProcessor:
      replicaCount: *nodeCount
      # If replicaFactor is increased, make sure to increase the
      # number of partitions for "backbeat-replication" topic
      # accordingly
      replicaFactor: 1
    statusProcessor:
      replicaCount: *nodeCount
  lifecycle:
    bucketProcessor:
      replicaCount: *nodeCount
    objectProcessor:
      replicaCount: *nodeCount
  garbageCollector:
    consumer:
      replicaCount: *nodeCount
  ingestion:
    enabled: true
    consumer:
      replicaCount: *nodeCount
  mongodb:
    replicas: *nodeCount

cosmos:
  enabled: true 
  ## Specify an existing storageClass to use
  # storageClass:
  rbac:
    create: true
    ## If set to true creates role and role binding instead of
    ## default clusterrole and clusterrole binding
    namespaced: false
  operator:
    image:
      repository: zenko/cosmos-operator
      tag: 0.5.5
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi
  scheduler:
    image:
      repository: zenko/cosmos-scheduler
      tag: 0.5.3
      pullPolicy: IfNotPresent

    ## A namespace to watch can be specified otherwise will default to the
    ## installed namespace
    # namespace: default
    schedule: "* */12 * * *"

    resources:
      limits:
        cpu: 50m
        memory: 64Mi

zenko-nfs:
  enabled: false
  mongodb:
    replicas: *nodeCount

prometheus:
  rbac:
    create: true
  alertmanager:
    enabled: false
  kubeStateMetrics:
    enabled: false
  nodeExporter:
    enabled: false
  pushgateway:
    enabled: false
  server:
    replicaCount: 2
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 5
          podAffinityTerm:
            topologyKey: "kubernetes.io/hostname"
            labelSelector:
              matchLabels:
                app: {{ template "prometheus.name" . }}
                release: {{ .Release.Name | quote }}
                component: server
  ## Adds a host_ip label to all the zenko pods
  ## to allow for metrics aggregation by node
  serverFiles:
    prometheus.yml:
      scrape_configs:
      - job_name: 'zenko-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: host_node
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

mongodb-replicaset:
  image:
    tag: 3.6.8
  replicaSetName: rs0
  replicas: *nodeCount
  podDisruptionBudget:
    maxUnavailable: 1
  metrics:
    enabled: true
  securityContext:
    runAsUser: 1000
    fsGroup: 1000
    runAsNonRoot: true
  persistentVolume:
    size: 50Gi


zenko-queue:
## Extensive list of configurables can be found here:
## https://github.com/kubernetes/charts/blob/master/incubator/kafka/values.yaml
  replicas: *nodeCount
  kafkaHeapOptions: "-Xmx6G -Xms1G"
  rbac:
    enabled: true
  configurationOverrides:
    "offsets.topic.replication.factor": 3 # - replication factor for the offsets topic
    "auto.create.topics.enable": false    # - enable auto creation of topic on the server
    "min.insync.replicas": 2              # - min number of replicas that must acknowledge a write
    "message.max.bytes": "5000000"        # - the largest record batch size allowed
  prometheus:
    jmx:
      enabled: true
    kafka:
      enabled: true
  topics:
    - name: backbeat-gc
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-ingestion
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-lifecycle-object-tasks
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-lifecycle-bucket-tasks
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-metrics
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-replication
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-replication-status
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-replication-failed
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-data-mover
      partitions: *nodeCount
      replicationFactor: 3
    - name: backbeat-sanitycheck
      partitions: 1
      replicationFactor: 3
  persistence:
    size: 20Gi

zenko-quorum:
## Extensive list of configurables can be found here:
## https://github.com/kubernetes/charts/blob/master/incubator/zookeeper/values.yaml
  replicaCount: *nodeCount
  exporters:
    jmx:
      enabled: true

redis-ha:
  replicas: *nodeCount
  redis:
    masterGroupName: zenko
    config:
      save: "60 1"
      stop-writes-on-bgsave-error: "yes"
      slave-serve-stale-data: "yes"
  sentinel:
    config:
      down-after-milliseconds: 5000
      failover-timeout: 60000

grafana:
  sidecar:
    image: zenko/grafana-sidecar:0.1
    datasources:
      enabled: true
      # Every config map with the following label will be used as datasource. By default,
      # promtheus is set as datasource under zenko/templates/datasources.yaml
      label: grafana-datasource
    dashboards:
      enabled: true
      # Likewise, every config map with the following label will be used as a source for
      # a dashboard.
      label: grafana-dashboard

zenko-queue-manager:
  zkHosts: "{{ .Release.Name }}-zenko-quorum-headless:2181"
  clusters:
    - name: "zenko-queue-cluster"
      tuning: {}

opal:
  blobserver:
    replicaCount: *nodeCount
  jabba:
    replicaCount: *nodeCount
